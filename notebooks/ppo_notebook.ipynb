{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **PPO agent Implementation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we gonna implement the PPO method to be able to compare if the agent is able to learn faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If on colab\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gym-super-mario-bros==7.4.0\n",
    "!pip install tensordict==0.3.0\n",
    "!pip install torchrl==0.3.0\n",
    "!pip install nes-py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Imports all needed libs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms as T\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Gym is an OpenAI toolkit for RL\n",
    "import gym\n",
    "from gym.spaces import Box\n",
    "from gym.wrappers import FrameStack\n",
    "\n",
    "# NES Emulator for OpenAI Gym\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "\n",
    "# Super Mario environment for OpenAI Gym\n",
    "import gym_super_mario_bros\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from gym.wrappers.monitoring import video_recorder\n",
    "\n",
    "from stable_baselines3 import PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Init and test of the env**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Super Mario environment (in v0.26 change render mode to 'human' to see results on the screen)\n",
    "print(gym.__version__)\n",
    "if gym.__version__ < '0.26':\n",
    "    env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\", new_step_api=True)\n",
    "else:\n",
    "    env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\", render_mode='rgb', apply_api_compatibility=True)\n",
    "\n",
    "# Limit the action-space to\n",
    "#   0. walk right\n",
    "#   1. jump right\n",
    "env = JoypadSpace(env, [[\"right\"], [\"right\", \"A\"]])\n",
    "\n",
    "env.reset()\n",
    "next_state, reward, done, trunc, info = env.step(action=0)\n",
    "print(f\"{next_state.shape},\\n {reward},\\n {done},\\n {info}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Preprocess of the env**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipFrame(gym.Wrapper):\n",
    "    def __init__(self, env, skip):\n",
    "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
    "        super().__init__(env)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Repeat action, and sum reward\"\"\"\n",
    "        total_reward = 0.0\n",
    "        for i in range(self._skip):\n",
    "            # Accumulate reward and repeat the same action\n",
    "            obs, reward, done, trunk, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        return obs, total_reward, done, trunk, info\n",
    "\n",
    "class GrayScaleObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        obs_shape = self.observation_space.shape[:2]\n",
    "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "\n",
    "    def permute_orientation(self, observation):\n",
    "        # permute [H, W, C] array to [C, H, W] tensor\n",
    "        observation = np.transpose(observation, (2, 0, 1))\n",
    "        observation = torch.tensor(observation.copy(), dtype=torch.float)\n",
    "        return observation\n",
    "\n",
    "    def observation(self, observation):\n",
    "        observation = self.permute_orientation(observation)\n",
    "        transform = T.Grayscale()\n",
    "        observation = transform(observation)\n",
    "        return observation\n",
    "\n",
    "\n",
    "class ResizeObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env, shape):\n",
    "        super().__init__(env)\n",
    "        if isinstance(shape, int):\n",
    "            self.shape = (shape, shape)\n",
    "        else:\n",
    "            self.shape = tuple(shape)\n",
    "\n",
    "        obs_shape = self.shape + self.observation_space.shape[2:]\n",
    "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        transforms = T.Compose(\n",
    "            [T.Resize(self.shape, antialias=True), T.Normalize(0, 255)]\n",
    "        )\n",
    "        observation = transforms(observation).squeeze(0)\n",
    "        return observation\n",
    "\n",
    "\n",
    "# Apply Wrappers to environment\n",
    "env = SkipFrame(env, skip=4)\n",
    "env = GrayScaleObservation(env)\n",
    "env = ResizeObservation(env, shape=84)\n",
    "if gym.__version__ < '0.26':\n",
    "    env = FrameStack(env, num_stack=4, new_step_api=True)\n",
    "else:\n",
    "    env = FrameStack(env, num_stack=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Definition of the Actor and Critic neural network**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Base Mlp policy**\n",
    "The graph of this policy is available in the reports/figures section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic_Mlp(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(ActorCritic_Mlp, self).__init__()\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(np.prod(state_dim), 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, action_dim),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(np.prod(state_dim), 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, state):\n",
    "        action_probs = self.actor(state)\n",
    "        value = self.critic(state)\n",
    "        return action_probs, value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Custom Cnn policy**\n",
    "The graph of this policy is available in the reports/figures section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic_Cnn(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(ActorCritic_Cnn, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(state_dim[0], 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        \n",
    "        conv_output_size = self._get_conv_output(state_dim)\n",
    "        \n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(conv_output_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, action_dim),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(conv_output_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "    \n",
    "    def _get_conv_output(self, shape):\n",
    "        x = torch.zeros(1, *shape)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        return int(np.prod(x.size()))\n",
    "    \n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.conv1(state))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        action_probs = self.actor(x)\n",
    "        value = self.critic(x)\n",
    "        return action_probs, value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Definition of the mario agent**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Definition of PPO for simple Mlp policy use**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO_Mlp:\n",
    "    def __init__(self, state_dim, action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        flattened_state_dim = np.prod(state_dim)  # Calculate the flattened state dimension\n",
    "        self.model = ActorCritic_Mlp(flattened_state_dim, action_dim).to(self.device)\n",
    "        self.optimizer = optim.Adam([\n",
    "            {'params': self.model.actor.parameters(), 'lr': lr_actor},\n",
    "            {'params': self.model.critic.parameters(), 'lr': lr_critic}\n",
    "        ])\n",
    "        self.gamma = gamma\n",
    "        self.K_epochs = K_epochs\n",
    "        self.eps_clip = eps_clip\n",
    "        self.memory = []\n",
    "        self.batch_size = 64\n",
    "   \n",
    "    def select_action(self, state):\n",
    "        state = state[0].__array__() if isinstance(state, tuple) else state.__array__()\n",
    "        state = torch.FloatTensor(state).flatten().unsqueeze(0).to(self.device)  # Flatten the state\n",
    "        action_probs, _ = self.model(state)\n",
    "        dist = Categorical(action_probs)\n",
    "        action = dist.sample()\n",
    "        return action.item()\n",
    "   \n",
    "    def update(self, memory):\n",
    "        states, actions, rewards, next_states, dones = zip(*self.memory)\n",
    "        states = np.array([state[0].__array__() if isinstance(state, tuple) else state.__array__() for state in states])\n",
    "        states = torch.FloatTensor(states).flatten(1).to(self.device)\n",
    "        actions = torch.LongTensor(actions).to(self.device)\n",
    "        rewards = torch.FloatTensor(rewards).to(self.device)\n",
    "        next_states = np.array([state[0].__array__() if isinstance(state, tuple) else state.__array__() for state in next_states])\n",
    "        next_states = torch.FloatTensor(next_states).flatten(1).to(self.device)\n",
    "        dones = torch.FloatTensor(dones).to(self.device)\n",
    "       \n",
    "        for _ in range(self.K_epochs):\n",
    "            action_probs, values = self.model(states)\n",
    "            dist = Categorical(action_probs)\n",
    "            action_log_probs = dist.log_prob(actions)\n",
    "           \n",
    "            _, next_values = self.model(next_states)\n",
    "            returns = self.compute_returns(rewards, next_values, dones)\n",
    "           \n",
    "            advantages = returns - values\n",
    "           \n",
    "            ratio = torch.exp(action_log_probs - action_log_probs.detach())\n",
    "            surr1 = ratio * advantages\n",
    "            surr2 = torch.clamp(ratio, 1-self.eps_clip, 1+self.eps_clip) * advantages\n",
    "            actor_loss = -torch.min(surr1, surr2).mean()\n",
    "           \n",
    "            critic_loss = 0.5 * (returns - values).pow(2).mean()\n",
    "           \n",
    "            self.optimizer.zero_grad()\n",
    "            (actor_loss + critic_loss).backward()\n",
    "            self.optimizer.step()\n",
    "   \n",
    "    def compute_returns(self, rewards, next_values, dones):\n",
    "        returns = torch.zeros_like(rewards)\n",
    "        running_return = next_values[-1]\n",
    "        for t in reversed(range(len(rewards))):\n",
    "            running_return = rewards[t] + self.gamma * running_return * (1 - dones[t])\n",
    "            returns[t] = running_return\n",
    "        return returns\n",
    "    \n",
    "    def save(self, path):\n",
    "        torch.save(self.model.state_dict(), path)\n",
    "\n",
    "    def load(self, path):\n",
    "        self.model.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Definition of PPO for Cnn policy use**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO_Cnn:\n",
    "    def __init__(self, state_dim, action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = ActorCritic_Cnn(state_dim, action_dim).to(self.device)\n",
    "        self.optimizer = optim.Adam([\n",
    "            {'params': self.model.actor.parameters(), 'lr': lr_actor},\n",
    "            {'params': self.model.critic.parameters(), 'lr': lr_critic}\n",
    "        ])\n",
    "        self.gamma = gamma\n",
    "        self.K_epochs = K_epochs\n",
    "        self.eps_clip = eps_clip\n",
    "        self.memory = []\n",
    "        self.batch_size = 64\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        state = state[0].__array__() if isinstance(state, tuple) else state.__array__()\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "        action_probs, _ = self.model(state)\n",
    "        dist = Categorical(action_probs)\n",
    "        action = dist.sample()\n",
    "        return action.item()\n",
    "    \n",
    "    def update(self, memory):\n",
    "        states, actions, rewards, next_states, dones = zip(*self.memory)\n",
    "        states = np.array([state[0].__array__() if isinstance(state, tuple) else state.__array__() for state in states])\n",
    "        states = torch.FloatTensor(states).to(self.device)\n",
    "        actions = torch.LongTensor(actions).to(self.device)\n",
    "        rewards = torch.FloatTensor(rewards).to(self.device)\n",
    "        next_states = np.array([state[0].__array__() if isinstance(state, tuple) else state.__array__() for state in next_states])\n",
    "        next_states = torch.FloatTensor(next_states).to(self.device)\n",
    "        dones = torch.FloatTensor(dones).to(self.device)\n",
    "        \n",
    "        for _ in range(self.K_epochs):\n",
    "            action_probs, values = self.model(states)\n",
    "            dist = Categorical(action_probs)\n",
    "            action_log_probs = dist.log_prob(actions)\n",
    "            \n",
    "            _, next_values = self.model(next_states)\n",
    "            returns = self.compute_returns(rewards, next_values, dones)\n",
    "            \n",
    "            advantages = returns - values\n",
    "            \n",
    "            ratio = torch.exp(action_log_probs - action_log_probs.detach())\n",
    "            surr1 = ratio * advantages\n",
    "            surr2 = torch.clamp(ratio, 1-self.eps_clip, 1+self.eps_clip) * advantages\n",
    "            actor_loss = -torch.min(surr1, surr2).mean()\n",
    "            \n",
    "            critic_loss = 0.5 * (returns - values).pow(2).mean()\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            (actor_loss + critic_loss).backward()\n",
    "            self.optimizer.step()\n",
    "    \n",
    "    def compute_returns(self, rewards, next_values, dones):\n",
    "        returns = torch.zeros_like(rewards)\n",
    "        running_return = next_values[-1]\n",
    "        for t in reversed(range(len(rewards))):\n",
    "            running_return = rewards[t] + self.gamma * running_return * (1 - dones[t])\n",
    "            returns[t] = running_return\n",
    "        return returns\n",
    "    \n",
    "    def save(self, path):\n",
    "        torch.save(self.model.state_dict(), path)\n",
    "    \n",
    "    def load(self, path):\n",
    "        self.model.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Definition of the training process**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving process:\n",
    "\n",
    "1- Save the model based on the average reward over a certain number of episodes instead of relying on a single episode's reward. This helps smooth out the variability and provides a more reliable measure of the agent's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env, agent, num_episodes, max_steps, save_path, save_interval=10):\n",
    "    info_list = []\n",
    "    best_mean_reward = -np.inf\n",
    "    with tqdm(total=num_episodes, desc=\"Training\", unit=\"episode\") as pbar:\n",
    "        for episode in range(num_episodes):\n",
    "            state = env.reset()\n",
    "            episode_reward = 0\n",
    "            \n",
    "            for step in range(max_steps):\n",
    "                action = agent.select_action(state)\n",
    "                next_state, reward, done, truncated, info = env.step(action)\n",
    "                \n",
    "                episode_reward += reward\n",
    "                \n",
    "                agent.memory.append((state, action, reward, next_state, done))\n",
    "                \n",
    "                if len(agent.memory) >= agent.batch_size:\n",
    "                    agent.update(agent.memory)\n",
    "                    agent.memory = []\n",
    "                \n",
    "                state = next_state\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "            \n",
    "            info_list.append([episode, episode_reward, step])\n",
    "\n",
    "            if episode % save_interval == 0 and episode > 0:\n",
    "                mean_reward = np.mean([info[1] for info in info_list[-save_interval:]])\n",
    "                if mean_reward > best_mean_reward:\n",
    "                    best_mean_reward = mean_reward\n",
    "                    agent.save(save_path)\n",
    "            \n",
    "            pbar.set_description(f\"Episode {episode+1} - Last Reward: {episode_reward:.2f}\")\n",
    "            pbar.update(1)\n",
    "    \n",
    "    return info_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_visualized(agent):\n",
    "   if gym.__version__ < '0.26':\n",
    "      env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\", new_step_api=True)\n",
    "   else:\n",
    "      env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\", render_mode='human', apply_api_compatibility=True)\n",
    "   \n",
    "   env = JoypadSpace(env, [[\"right\"], [\"right\", \"A\"]])\n",
    "\n",
    "   env = SkipFrame(env, skip=4)\n",
    "   env = GrayScaleObservation(env)\n",
    "   env = ResizeObservation(env, shape=84)\n",
    "   if gym.__version__ < '0.26':\n",
    "      env = FrameStack(env, num_stack=4, new_step_api=True)\n",
    "   else:\n",
    "      env = FrameStack(env, num_stack=4)\n",
    "   \n",
    "   state = env.reset()\n",
    "   done = False\n",
    "\n",
    "   while done == False:\n",
    "      action = agent.select_action(state)\n",
    "      state, reward, done, truncated, info = env.step(action)\n",
    "      env.render()\n",
    "      \n",
    "   env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dim = env.observation_space.shape\n",
    "action_dim = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"state_dim = \", state_dim)\n",
    "print(\"action_dim = \", action_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Train with the mlp policy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the PPO agent for mlp\n",
    "agent = PPO_Mlp(state_dim, action_dim, lr_actor=0.0003, lr_critic=0.0003, gamma=0.99, K_epochs=10, eps_clip=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the agent\n",
    "num_episodes = 15000\n",
    "max_steps = 2000\n",
    "save_path = \"../best_agent/best_agent_ppo_mlp.pth\"\n",
    "\n",
    "info_list_mlp = train(env, agent, num_episodes, max_steps, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Plot information about the training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert info_list to a DataFrame\n",
    "df = pd.DataFrame(info_list_mlp, columns=[\"Episode\", \"Reward\", \"Steps\"])\n",
    "\n",
    "# Plot the reward evolution by episode\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df[\"Episode\"], df[\"Reward\"])\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.title(\"Reward Evolution\")\n",
    "\n",
    "# Calculate the smoothed reward using a rolling window of size 10\n",
    "smoothed_reward = df[\"Reward\"].rolling(window=500).mean()\n",
    "\n",
    "# Plot the smoothed reward evolution\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df[\"Episode\"], smoothed_reward)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Smoothed Reward\")\n",
    "plt.title(\"Smoothed Reward Evolution (Window Size: 10)\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Train with the Cnn policy**\n",
    "From here we have implement from scratch the PPO and Mlp policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the PPO agent for cnn\n",
    "agent = PPO_Cnn(state_dim, action_dim, lr_actor=0.0003, lr_critic=0.0003, gamma=0.99, K_epochs=10, eps_clip=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 2 - Last Reward: 807.00:   0%|          | 2/15000 [00:09<17:27:52,  4.19s/episode]"
     ]
    }
   ],
   "source": [
    "# Train the agent\n",
    "num_episodes = 15000\n",
    "max_steps = 2000\n",
    "save_path = \"../best_agent/best_agent_ppo_cnn.pth\"\n",
    "\n",
    "info_list_cnn = train(env, agent, num_episodes, max_steps, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert info_list to a DataFrame\n",
    "df = pd.DataFrame(info_list_cnn, columns=[\"Episode\", \"Reward\", \"Steps\"])\n",
    "\n",
    "# Plot the reward evolution by episode\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df[\"Episode\"], df[\"Reward\"])\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.title(\"Reward Evolution\")\n",
    "\n",
    "# Calculate the smoothed reward using a rolling window of size 10\n",
    "smoothed_reward = df[\"Reward\"].rolling(window=500).mean()\n",
    "\n",
    "# Plot the smoothed reward evolution\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df[\"Episode\"], smoothed_reward)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Smoothed Reward\")\n",
    "plt.title(\"Smoothed Reward Evolution (Window Size: 10)\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Test the best agent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best agent\n",
    "best_agent_path = best_agent_dir / \"best_agent.chkpt\"\n",
    "agent.load(best_agent_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the agent\n",
    "test_visualized(agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
