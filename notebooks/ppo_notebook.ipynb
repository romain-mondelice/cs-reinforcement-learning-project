{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Double DQN agent Implementation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we gonna implement the DDQN method to be able to compare if the agent is able to learn faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If on colab\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gym-super-mario-bros==7.4.0\n",
    "!pip install tensordict==0.3.0\n",
    "!pip install torchrl==0.3.0\n",
    "!pip install nes-py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Imports all needed libs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms as T\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import pandas as pd\n",
    "\n",
    "# Gym is an OpenAI toolkit for RL\n",
    "import gym\n",
    "from gym.spaces import Box\n",
    "from gym.wrappers import FrameStack\n",
    "\n",
    "# NES Emulator for OpenAI Gym\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "\n",
    "# Super Mario environment for OpenAI Gym\n",
    "import gym_super_mario_bros\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from gym.wrappers.monitoring import video_recorder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Init and test of the env**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Super Mario environment (in v0.26 change render mode to 'human' to see results on the screen)\n",
    "print(gym.__version__)\n",
    "if gym.__version__ < '0.26':\n",
    "    env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\", new_step_api=True)\n",
    "else:\n",
    "    env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\", render_mode='rgb', apply_api_compatibility=True)\n",
    "\n",
    "# Limit the action-space to\n",
    "#   0. walk right\n",
    "#   1. jump right\n",
    "env = JoypadSpace(env, [[\"right\"], [\"right\", \"A\"]])\n",
    "\n",
    "env.reset()\n",
    "next_state, reward, done, trunc, info = env.step(action=0)\n",
    "print(f\"{next_state.shape},\\n {reward},\\n {done},\\n {info}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Preprocess of the env**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipFrame(gym.Wrapper):\n",
    "    def __init__(self, env, skip):\n",
    "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
    "        super().__init__(env)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Repeat action, and sum reward\"\"\"\n",
    "        total_reward = 0.0\n",
    "        for i in range(self._skip):\n",
    "            # Accumulate reward and repeat the same action\n",
    "            obs, reward, done, trunk, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        return obs, total_reward, done, trunk, info\n",
    "\n",
    "\n",
    "class GrayScaleObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        obs_shape = self.observation_space.shape[:2]\n",
    "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "\n",
    "    def permute_orientation(self, observation):\n",
    "        # permute [H, W, C] array to [C, H, W] tensor\n",
    "        observation = np.transpose(observation, (2, 0, 1))\n",
    "        observation = torch.tensor(observation.copy(), dtype=torch.float)\n",
    "        return observation\n",
    "\n",
    "    def observation(self, observation):\n",
    "        observation = self.permute_orientation(observation)\n",
    "        transform = T.Grayscale()\n",
    "        observation = transform(observation)\n",
    "        return observation\n",
    "\n",
    "\n",
    "class ResizeObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env, shape):\n",
    "        super().__init__(env)\n",
    "        if isinstance(shape, int):\n",
    "            self.shape = (shape, shape)\n",
    "        else:\n",
    "            self.shape = tuple(shape)\n",
    "\n",
    "        obs_shape = self.shape + self.observation_space.shape[2:]\n",
    "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        transforms = T.Compose(\n",
    "            [T.Resize(self.shape, antialias=True), T.Normalize(0, 255)]\n",
    "        )\n",
    "        observation = transforms(observation).squeeze(0)\n",
    "        return observation\n",
    "\n",
    "\n",
    "# Apply Wrappers to environment\n",
    "env = SkipFrame(env, skip=4)\n",
    "env = GrayScaleObservation(env)\n",
    "env = ResizeObservation(env, shape=84)\n",
    "if gym.__version__ < '0.26':\n",
    "    env = FrameStack(env, num_stack=4, new_step_api=True)\n",
    "else:\n",
    "    env = FrameStack(env, num_stack=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Definition of the Actor and Critic neural network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(np.prod(state_dim), 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, action_dim),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(np.prod(state_dim), 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, state):\n",
    "        action_probs = self.actor(state)\n",
    "        value = self.critic(state)\n",
    "        return action_probs, value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Definition of the mario agent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO:\n",
    "    def __init__(self, state_dim, action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        flattened_state_dim = np.prod(state_dim)  # Calculate the flattened state dimension\n",
    "        self.model = ActorCritic(flattened_state_dim, action_dim).to(self.device)\n",
    "        self.optimizer = optim.Adam([\n",
    "            {'params': self.model.actor.parameters(), 'lr': lr_actor},\n",
    "            {'params': self.model.critic.parameters(), 'lr': lr_critic}\n",
    "        ])\n",
    "        self.gamma = gamma\n",
    "        self.K_epochs = K_epochs\n",
    "        self.eps_clip = eps_clip\n",
    "        self.memory = []\n",
    "        self.batch_size = 64\n",
    "   \n",
    "    def select_action(self, state):\n",
    "        state = state[0].__array__() if isinstance(state, tuple) else state.__array__()\n",
    "        state = torch.FloatTensor(state).flatten().unsqueeze(0).to(self.device)  # Flatten the state\n",
    "        action_probs, _ = self.model(state)\n",
    "        dist = Categorical(action_probs)\n",
    "        action = dist.sample()\n",
    "        return action.item()\n",
    "   \n",
    "    def update(self, memory):\n",
    "        states, actions, rewards, next_states, dones = zip(*self.memory)\n",
    "        states = np.array([state[0].__array__() if isinstance(state, tuple) else state.__array__() for state in states])\n",
    "        states = torch.FloatTensor(states).flatten(1).to(self.device)\n",
    "        actions = torch.LongTensor(actions).to(self.device)\n",
    "        rewards = torch.FloatTensor(rewards).to(self.device)\n",
    "        next_states = np.array([state[0].__array__() if isinstance(state, tuple) else state.__array__() for state in next_states])\n",
    "        next_states = torch.FloatTensor(next_states).flatten(1).to(self.device)\n",
    "        dones = torch.FloatTensor(dones).to(self.device)\n",
    "       \n",
    "        for _ in range(self.K_epochs):\n",
    "            action_probs, values = self.model(states)\n",
    "            dist = Categorical(action_probs)\n",
    "            action_log_probs = dist.log_prob(actions)\n",
    "           \n",
    "            _, next_values = self.model(next_states)\n",
    "            returns = self.compute_returns(rewards, next_values, dones)\n",
    "           \n",
    "            advantages = returns - values\n",
    "           \n",
    "            ratio = torch.exp(action_log_probs - action_log_probs.detach())\n",
    "            surr1 = ratio * advantages\n",
    "            surr2 = torch.clamp(ratio, 1-self.eps_clip, 1+self.eps_clip) * advantages\n",
    "            actor_loss = -torch.min(surr1, surr2).mean()\n",
    "           \n",
    "            critic_loss = 0.5 * (returns - values).pow(2).mean()\n",
    "           \n",
    "            self.optimizer.zero_grad()\n",
    "            (actor_loss + critic_loss).backward()\n",
    "            self.optimizer.step()\n",
    "   \n",
    "    def compute_returns(self, rewards, next_values, dones):\n",
    "        returns = torch.zeros_like(rewards)\n",
    "        running_return = next_values[-1]\n",
    "        for t in reversed(range(len(rewards))):\n",
    "            running_return = rewards[t] + self.gamma * running_return * (1 - dones[t])\n",
    "            returns[t] = running_return\n",
    "        return returns\n",
    "    \n",
    "    def save(self, path):\n",
    "        torch.save(self.model.state_dict(), path)\n",
    "\n",
    "    def load(self, path):\n",
    "        self.model.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metric class to print and save usefull insight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Definition of the training process**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving process:\n",
    "\n",
    "1- Save the model based on the average reward over a certain number of episodes instead of relying on a single episode's reward. This helps smooth out the variability and provides a more reliable measure of the agent's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env, agent, num_episodes, max_steps, save_path, save_interval=10):\n",
    "    info_list = []\n",
    "    best_mean_reward = -np.inf\n",
    "    with tqdm(total=num_episodes, desc=\"Training\", unit=\"episode\") as pbar:\n",
    "        for episode in range(num_episodes):\n",
    "            state = env.reset()\n",
    "            episode_reward = 0\n",
    "            \n",
    "            for step in range(max_steps):\n",
    "                action = agent.select_action(state)\n",
    "                next_state, reward, done, truncated, info = env.step(action)\n",
    "                \n",
    "                episode_reward += reward\n",
    "                \n",
    "                agent.memory.append((state, action, reward, next_state, done))\n",
    "                \n",
    "                if len(agent.memory) >= agent.batch_size:\n",
    "                    agent.update(agent.memory)\n",
    "                    agent.memory = []\n",
    "                \n",
    "                state = next_state\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "            \n",
    "            info_list.append([episode, episode_reward, step])\n",
    "\n",
    "            if episode % save_interval == 0 and episode > 0:\n",
    "                mean_reward = np.mean([info[1] for info in info_list[-save_interval:]])\n",
    "                if mean_reward > best_mean_reward:\n",
    "                    best_mean_reward = mean_reward\n",
    "                    agent.save(save_path)\n",
    "            \n",
    "            pbar.set_description(f\"Episode {episode+1} - Last Reward: {episode_reward:.2f}\")\n",
    "            pbar.update(1)\n",
    "    \n",
    "    return info_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_visualized(agent):\n",
    "   if gym.__version__ < '0.26':\n",
    "      env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\", new_step_api=True)\n",
    "   else:\n",
    "      env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\", render_mode='human', apply_api_compatibility=True)\n",
    "   \n",
    "   env = JoypadSpace(env, [[\"right\"], [\"right\", \"A\"]])\n",
    "\n",
    "   env = SkipFrame(env, skip=4)\n",
    "   env = GrayScaleObservation(env)\n",
    "   env = ResizeObservation(env, shape=84)\n",
    "   if gym.__version__ < '0.26':\n",
    "      env = FrameStack(env, num_stack=4, new_step_api=True)\n",
    "   else:\n",
    "      env = FrameStack(env, num_stack=4)\n",
    "   \n",
    "   state = env.reset()\n",
    "   done = False\n",
    "\n",
    "   while done == False:\n",
    "      action = agent.select_action(state)\n",
    "      state, reward, done, truncated, info = env.step(action)\n",
    "      env.render()\n",
    "      \n",
    "   env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dim = env.observation_space.shape\n",
    "action_dim = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the PPO agent\n",
    "agent = PPO(state_dim, action_dim, lr_actor=0.0003, lr_critic=0.0003, gamma=0.99, K_epochs=10, eps_clip=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the agent\n",
    "num_episodes = 100\n",
    "max_steps = 10000\n",
    "save_path = \"../best_agent/best_model.pth\"\n",
    "\n",
    "info_list = train(env, agent, num_episodes, max_steps, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Plot information about the training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert info_list to a DataFrame\n",
    "df = pd.DataFrame(info_list, columns=[\"Episode\", \"Reward\", \"Steps\"])\n",
    "\n",
    "# Plot the reward evolution by episode\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df[\"Episode\"], df[\"Reward\"])\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.title(\"Reward Evolution\")\n",
    "\n",
    "# Calculate the smoothed reward using a rolling window of size 10\n",
    "smoothed_reward = df[\"Reward\"].rolling(window=10).mean()\n",
    "\n",
    "# Plot the smoothed reward evolution\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df[\"Episode\"], smoothed_reward)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Smoothed Reward\")\n",
    "plt.title(\"Smoothed Reward Evolution (Window Size: 10)\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Test the best agent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best agent\n",
    "best_agent_path = best_agent_dir / \"best_agent.chkpt\"\n",
    "agent.load(best_agent_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the agent\n",
    "test_visualized(agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
